{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85913a0b",
   "metadata": {},
   "source": [
    "# Automatic summarisation us Tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a563e2",
   "metadata": {},
   "source": [
    "Using TF-IDF\n",
    "This method uses the frequency of words in the document and thus making the summary depending on the frequency.\n",
    "Although many advancement had happened in tf-idf for summarisation but a general model works like-\n",
    "1. Tokenize the sentences\n",
    " 2. Create the Frequency matrix of the words in each sentence.\n",
    "3. Calculate TermFrequency and generate a matrix\n",
    "4. Creating a table for documents per words\n",
    "5. Calculate IDF and generate a matrix\n",
    "6. Calculate TF-IDF and generate a matrix\n",
    "7. Score the sentences\n",
    "8. Find the threshold\n",
    "9. Generate the summary\n",
    "\n",
    "The threshold for the the tf-idf can be set by many methods just as average or median.\n",
    "Limitation:\n",
    "Creates extractive summary only\n",
    "Do not include semantic knowledge and other aspects of text.\n",
    "Expansive on the memory and the processor\n",
    "Redundancy in the document can result to wrong evaluation\n",
    "\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d118d",
   "metadata": {},
   "source": [
    "### import the library used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce39aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ae176",
   "metadata": {},
   "source": [
    "### Use of spacy\n",
    "In the task there is a need to divide the text into sentences hence we will be using Spacy library for the same.\n",
    "We prefer usin spacy as it it one of the fastest nlp library available along with the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63677927",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc5535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"welcome.txt\", \"r\")\n",
    "f.seek(0)\n",
    "text=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a027c",
   "metadata": {},
   "source": [
    "# Making Sentences of Text\n",
    "by using spacy we will convert the text into sentences to create tf_isf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f962773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_list(txt):\n",
    "    doc=nlp(txt)\n",
    "    sentence_list=list()\n",
    "    for i in doc.sents:\n",
    "        sentence_list.append(i.text)\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b8a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "start=default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1968b4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14152859999999912"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st1=default_timer()\n",
    "df=sent_list(text.lower())\n",
    "sto1=default_timer()\n",
    "sto1-st1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f99875",
   "metadata": {},
   "source": [
    "# Using SciKit Learn Tf_idf Vecorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73f982e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "a=TfidfVectorizer(stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "474c6762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004120799999999036"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st1=default_timer()\n",
    "b=a.fit_transform(df)\n",
    "sto1=default_timer()\n",
    "sto1-st1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f88f1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=b.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ba9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg=np.percentile(z, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57101c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing (nlp) is an important tool for understanding and\n",
      "processing the immense volume of unstructured data in today's world.\n",
      "recently,\n",
      "deep learning has been widely adopted for many nlp tasks because of the\n",
      "remarkable performance that deep learning algorithms have shown in a plethora\n",
      "of challenging tasks, such as, image classification, speech recognition, and realistic\n",
      "text generation.\n",
      "this book will enable aspiring deep\n",
      "learning developers to handle massive amounts of data using nlp and tensorflow.\n",
      "we will also consider the traditional\n",
      "approaches and the more recent deep learning-based approaches to nlp, including\n",
      "a fully-connected neural network (fcnn).\n",
      "of all this data,\n",
      "a large fraction is unstructured text and speech as there are millions of emails and\n",
      "social media content created and phone calls made every day.\n",
      "next, it must understand the semantics of the request\n",
      "(for example, the user is looking for a good restaurant with an italian cuisine) and\n",
      "formulate a structured request (for example, cuisine = italian, rating =\n",
      "\n",
      "it should be understood that nlp is an extremely challenging field of research as\n",
      "words and semantics have a highly complex nonlinear relationship, and it is even\n",
      "more difficult to capture this information as a robust numerical representation.\n",
      "\n",
      "therefore, processing textual data involves various complex tasks such as text\n",
      "parsing (for example, tokenization and stemming), morphological analysis, word\n",
      "sense disambiguation, and understanding the underlying grammatical structure of\n",
      "a language.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(z)):\n",
    "    if z[i]>avg:\n",
    "        print(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee54f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=default_timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bdb0e",
   "metadata": {},
   "source": [
    "Total time took by the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "627513dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25012420000000013\n"
     ]
    }
   ],
   "source": [
    "print(stop-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
